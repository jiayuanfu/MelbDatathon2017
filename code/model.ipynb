{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading data for training models\n",
    "\n",
    "- The original data has been processed with MySQL for extract features.\n",
    "- Instead of using MySQL, pymssql or pandas (in parallel) also can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# loading training data, seperate data and label\n",
    "def load_data(filepath):\n",
    "    data = []\n",
    "    label = []\n",
    "    with open(filepath) as f:\n",
    "        f_csv = csv.reader(f)\n",
    "        headers = next(f_csv)\n",
    "        for row in f_csv: \n",
    "            row_data = []\n",
    "            for item in row[2:9]:\n",
    "                row_data.append(int(re.sub(r',','',item)))\n",
    "            data.append(row_data)\n",
    "            label.append(int(row[9]))   \n",
    "    return data,label,headers\n",
    "\n",
    "# loading final data set for prediction, return data and ID\n",
    "def load_data_testfinal(filepath):\n",
    "    ID = []\n",
    "    data = []\n",
    "    with open(filepath) as f:\n",
    "        f_csv = csv.reader(f)\n",
    "        headers = next(f_csv)\n",
    "        for row in f_csv: \n",
    "            row_data = []\n",
    "            ID.append(int(re.sub(r',','',row[0])))\n",
    "            for item in row[2:9]:\n",
    "                row_data.append(int(re.sub(r',','',item)))\n",
    "            data.append(row_data)\n",
    "    return data,ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data,train_label,train_headers = load_data('./data/train/TRAIN.csv')\n",
    "test_data,test_label,test_headers = load_data('./data/train/DEV1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features which have been used for training models are: \n",
      "['year_of_birth', 'state_code', 'trans_all', 'trans_Lipids', 'trans_Hypertension', 'trans_Depression', 'trans_Diabetes']\n",
      "[1900, 2, 92, 12, 46, 0, 6]\n"
     ]
    }
   ],
   "source": [
    "print\"The features which have been used for training models are: \"\n",
    "print train_headers[2:9]\n",
    "print train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comments for potential improvement:\n",
    "- Extract more features for describing instances more in detail and improving the performance of the model.\n",
    "- Add data augment to increase the size of training set for avoiding overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The evaluation of potential base models\n",
    "- The models have been used: Naive Bayes, KNN, Logistic Regression, Decision Tree.\n",
    "- Algorithm tuning: grid search.\n",
    "- Measurement: AUC and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "AUC: 0.904442768446\n",
      "Accuracy: 0.918\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "probabilities = []\n",
    "model = GaussianNB()\n",
    "\n",
    "# predict the label\n",
    "model.fit(train_data,train_label)\n",
    "predictions = list(model.predict(test_data))\n",
    "\n",
    "# pick up the probability of prediction\n",
    "proba = model.predict_proba(test_data)\n",
    "for p in proba:\n",
    "    probabilities.append(p[1])\n",
    "\n",
    "# calculate auc\n",
    "auc = roc_auc_score(test_label,probabilities)\n",
    "\n",
    "print model\n",
    "print 'AUC:',auc\n",
    "print \"Accuracy:\",accuracy_score(test_label,predictions)\n",
    "# print classification_report(test_label,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "AUC: 0.943176414249\n",
      "Accuracy: 0.9335\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "AUC: 0.943395832881\n",
      "Accuracy: 0.9335\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=50, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "AUC: 0.942795143089\n",
      "Accuracy: 0.9335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def evaluation_metric_KNN(train_data,train_label,test_data,test_label):\n",
    "    # set up parameters for grid search\n",
    "    c_to_test = [10,30,50]\n",
    "    \n",
    "    models = [KNeighborsClassifier(leaf_size = c) for c in c_to_test]\n",
    "\n",
    "    for model in models:\n",
    "        probabilities = []\n",
    "        \n",
    "        # predict the label\n",
    "        model.fit(train_data,train_label)\n",
    "        predictions = list(model.predict(test_data))\n",
    "\n",
    "        # pick up the probability of prediction\n",
    "        proba = model.predict_proba(test_data)\n",
    "        for p in proba:\n",
    "            probabilities.append(p[1])\n",
    "            \n",
    "        # calculate auc\n",
    "        auc = roc_auc_score(test_label,probabilities)\n",
    "        \n",
    "        print model\n",
    "        print 'AUC:',auc\n",
    "        print \"Accuracy:\",accuracy_score(test_label,predictions)\n",
    "#         print classification_report(test_label,predictions)\n",
    "        print\n",
    "        \n",
    "evaluation_metric_KNN(train_data,train_label,test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "AUC: 0.955553126841\n",
      "Accuracy: 0.9225\n",
      "\n",
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.001,\n",
      "          verbose=0, warm_start=False)\n",
      "AUC: 0.954630400577\n",
      "Accuracy: 0.9235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def evaluation_metric_LR(train_data,train_label,test_data,test_label):\n",
    "    # set up parameters for grid search\n",
    "    c_to_test = [0.0001,0.001]\n",
    "    \n",
    "    models = [LogisticRegression(C = 0.1, intercept_scaling=1, dual=False, fit_intercept=True, penalty='l2', tol=c) for c in c_to_test]\n",
    "\n",
    "    for model in models:\n",
    "        probabilities = []\n",
    "        \n",
    "        # predict the label\n",
    "        model.fit(train_data,train_label)\n",
    "        predictions = list(model.predict(test_data))\n",
    "\n",
    "        # pick up the probability of prediction\n",
    "        proba = model.predict_proba(test_data)\n",
    "        for p in proba:\n",
    "            probabilities.append(p[1])\n",
    "            \n",
    "        # calculate auc\n",
    "        auc = roc_auc_score(test_label,probabilities)\n",
    "        \n",
    "        print model\n",
    "        print 'AUC:',auc\n",
    "        print \"Accuracy:\",accuracy_score(test_label,predictions)\n",
    "#         print classification_report(test_label,predictions)\n",
    "        print\n",
    "        \n",
    "evaluation_metric_LR(train_data,train_label,test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=5, max_leaf_nodes=None, min_samples_leaf=3,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=9334, splitter='best')\n",
      "AUC: 0.963508929421\n",
      "Accuracy: 0.9445\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=5, max_leaf_nodes=None, min_samples_leaf=3,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=12343, splitter='best')\n",
      "AUC: 0.963488072136\n",
      "Accuracy: 0.9405\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=5, max_leaf_nodes=None, min_samples_leaf=3,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=6217, splitter='best')\n",
      "AUC: 0.963275327835\n",
      "Accuracy: 0.9435\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=5, max_leaf_nodes=None, min_samples_leaf=3,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=7307, splitter='best')\n",
      "AUC: 0.96326615063\n",
      "Accuracy: 0.942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def evaluation_metric_tree(train_data,train_label,test_data,test_label):\n",
    "    # set up parameters for grid search\n",
    "#     d_to_test = [5,6]\n",
    "#     f_to_test = [5,6] \n",
    "#     c_to_test = [1,2,3]\n",
    "#     ms_to_test = [7,10]\n",
    "    s_to_test = [9334,12343,6217,7307]\n",
    "    \n",
    "#     models = [DecisionTreeClassifier(max_depth = d, max_features = f, min_samples_split = ms, min_samples_leaf = c, random_state = s) for d in d_to_test for f in f_to_test for ms in ms_to_test for c in c_to_test for s in s_to_test]\n",
    "    models = [DecisionTreeClassifier(max_depth = 5, max_features = 5, min_samples_split = 10, min_samples_leaf = 3, random_state = s) for s in s_to_test]\n",
    "    \n",
    "    for model in models:\n",
    "        probabilities = []\n",
    "        \n",
    "        # predict the label\n",
    "        model.fit(train_data,train_label)\n",
    "        predictions = list(model.predict(test_data))\n",
    "        \n",
    "        # pick up the probability of prediction\n",
    "        proba = model.predict_proba(test_data)\n",
    "        for p in proba:\n",
    "            probabilities.append(p[1])\n",
    "            \n",
    "        # calculate auc\n",
    "        auc = roc_auc_score(test_label,probabilities)\n",
    "        \n",
    "        if auc > 0.963:\n",
    "            print model  \n",
    "            print 'AUC:',auc\n",
    "            print \"Accuracy:\",accuracy_score(test_label,predictions)\n",
    "    #         print classification_report(test_label,predictions)\n",
    "            print\n",
    "        \n",
    "evaluation_metric_tree(train_data,train_label,test_data,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "- Performance: NB < KNN < LR < Decision Tree.\n",
    "- NB: Generative classifier, features have to be strictly independent.\n",
    "- KNN: When the classification is unbalanced, the performance is poor.\n",
    "- LR: Hard to handle nonlinear relationships between features.\n",
    "- Decision Tree: Consider the interaction between features, able to handle nonlinear relationships between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ensemble methods\n",
    "- Boosting: Gradient boosting.\n",
    "- Stacking: simple voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(init=None, learning_rate=1.0, loss='deviance',\n",
      "              max_depth=5, max_features=5, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=10,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=124, subsample=0.8, verbose=0,\n",
      "              warm_start=False)\n",
      "AUC: 0.962204097706\n",
      "Accuracy: 0.9425\n",
      "\n",
      "GradientBoostingClassifier(init=None, learning_rate=1.0, loss='deviance',\n",
      "              max_depth=5, max_features=5, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=10,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=179, subsample=0.8, verbose=0,\n",
      "              warm_start=False)\n",
      "AUC: 0.961936290173\n",
      "Accuracy: 0.9445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def evaluation_metric_GBDT(train_data,train_label,test_data,test_label):\n",
    "    # grid search\n",
    "    s_to_test = [124,179]\n",
    "    \n",
    "#     models = [GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=5, max_features=5, min_samples_split=10, subsample=0.8, random_state = s) for s in xrange(100,300)]\n",
    "    models = [GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=5, max_features=5, min_samples_split=10, subsample=0.8, random_state = s) for s in s_to_test]\n",
    "    \n",
    "    for model in models:\n",
    "        probabilities = []\n",
    "        \n",
    "        # predict the label\n",
    "        model.fit(train_data,train_label)\n",
    "        predictions = list(model.predict(test_data))\n",
    "        \n",
    "        # pick up the probability of prediction\n",
    "        proba = model.predict_proba(test_data)\n",
    "        for p in proba:\n",
    "            probabilities.append(p[1])\n",
    "            \n",
    "        # calculate auc\n",
    "        auc = roc_auc_score(test_label,probabilities)\n",
    "        \n",
    "        if auc > 0.96:\n",
    "            print model  \n",
    "            print 'AUC:',auc\n",
    "            print \"Accuracy:\",accuracy_score(test_label,predictions)\n",
    "    #         print classification_report(test_label,predictions)\n",
    "            print\n",
    "        \n",
    "evaluation_metric_GBDT(train_data,train_label,test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('tree9334', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=5, max_leaf_nodes=None, min_samples_leaf=2,\n",
      "            min_samples_split=7, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=9334, splitter='best')), ('tre...t=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=7307, splitter='best'))],\n",
      "         voting='soft', weights=None)\n",
      "AUC: 0.964480044585\n",
      "Accuracy: 0.9455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = []\n",
    "model1 = DecisionTreeClassifier(max_depth = 5, max_features = 5, min_samples_leaf = 2, min_samples_split = 7, random_state = 9334)\n",
    "estimators.append(('tree9334', model1))\n",
    "model2 = DecisionTreeClassifier(max_depth = 5, max_features = 5, min_samples_leaf = 3, min_samples_split = 10, random_state = 12343)\n",
    "estimators.append(('tree12343', model2))\n",
    "model3 = DecisionTreeClassifier(max_depth = 5, max_features = 5, min_samples_leaf = 2, min_samples_split = 7, random_state = 6217)\n",
    "estimators.append(('tree6217', model3))\n",
    "model4 = DecisionTreeClassifier(max_depth = 5, max_features = 5, min_samples_leaf = 3, min_samples_split = 10, random_state = 7307)\n",
    "estimators.append(('tree7307', model4))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators,voting='soft')\n",
    "\n",
    "probabilities = []\n",
    "\n",
    "# predict the label\n",
    "ensemble.fit(train_data,train_label)\n",
    "predictions = list(ensemble.predict(test_data))\n",
    "print ensemble\n",
    "\n",
    "# # pick up the probability of prediction\n",
    "proba = ensemble.predict_proba(test_data)\n",
    "for p in proba:\n",
    "    probabilities.append(p[1])\n",
    "\n",
    "# calculate auc\n",
    "auc = roc_auc_score(test_label,probabilities)\n",
    "\n",
    "# print model\n",
    "print 'AUC:',auc\n",
    "print \"Accuracy:\",accuracy_score(test_label,predictions)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "- Performance: Boosting < Stacking.\n",
    "- Boosting: Iterative sampling, target \"hard\" instances. However, the influence is limited.\n",
    "- Stacking: Multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final prediction\n",
    "The final model is the stacking method above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_final_data,train_final_label,train_final_headers = load_data('./data/train/TRAIN_Final.csv')\n",
    "test_final_data,test_final_ID = load_data_testfinal('./data/train/TEST_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ensemble\n",
    "probabilities = []\n",
    "model.fit(train_data,train_label)\n",
    "proba = model.predict_proba(test_final_data)\n",
    "for p in proba:\n",
    "    probabilities.append(p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions.\n"
     ]
    }
   ],
   "source": [
    "headers = ['Patient_ID','Diabetes']\n",
    "rows = []\n",
    "for i in range(len(probabilities)):\n",
    "    item = (test_final_ID[i],probabilities[i])\n",
    "    rows.append(item)\n",
    "\n",
    "with open(\"./data/prediction/preds.csv\", \"w\") as f:    \n",
    "    preds_csv = csv.writer(f)\n",
    "    preds_csv.writerow(headers)\n",
    "    preds_csv.writerows(rows)\n",
    "print(\"Wrote predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
